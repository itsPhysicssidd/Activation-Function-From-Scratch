{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd18497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the input range\n",
    "input_values = np.linspace(-2, 2, 100)\n",
    "\n",
    "# Apply weight and bias\n",
    "weight = -1\n",
    "bias = 0.5\n",
    "adjusted_input = weight * input_values + bias\n",
    "\n",
    "# Apply ReLU activation function\n",
    "relu_output = np.maximum(0, adjusted_input)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(input_values, relu_output, label='ReLU Activation', color='blue')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.scatter([-1], [np.maximum(0, weight * -1 + bias)], color='red', label='Input = -1', zorder=5)\n",
    "plt.title(\"ReLU Activation Function (Weight = -1, Bias = 0.5)\", fontsize=14)\n",
    "plt.xlabel(\"Input\", fontsize=12)\n",
    "plt.ylabel(\"Output\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376359e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input range\n",
    "input_values = np.linspace(-2, 2, 100)\n",
    "\n",
    "# Apply weight and bias\n",
    "weight = -1\n",
    "bias = 0.5\n",
    "adjusted_input = weight * input_values + bias\n",
    "\n",
    "# Apply ReLU activation function\n",
    "relu_output = np.maximum(0, adjusted_input)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(input_values, relu_output, label='ReLU Activation', color='blue')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.scatter([-1], [np.maximum(0, weight * -1 + bias)], color='red', label='Input = -1', zorder=5)\n",
    "plt.title(\"ReLU Activation Function (Weight = -1, Bias = 0.5)\", fontsize=14)\n",
    "plt.xlabel(\"Input\", fontsize=12)\n",
    "plt.ylabel(\"Output\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cd210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Demonstrating one pair of neuron: assume 2 hidden layers of 1 neuron each with weight -1 and bias 0.5 of first neuron and weight 1 and bias 0 of second neuron\n",
    "\n",
    "#step1 : Initializing the weights and bias of neuron 1 and neuron 2\n",
    "weight_neuron1 = -1.00\n",
    "bias_neuron1 = 0.5\n",
    "weight_neuron2 = 1.00\n",
    "bias_neuron2 = 0.00\n",
    "\n",
    "#step 2: input values \n",
    "input_values = np.linspace(-2, 2, 100)\n",
    "\n",
    "# Step 3: First neuron computation\n",
    "adjusted_input_neuron1 = weight_neuron1 * input_values + bias_neuron1\n",
    "relu_output_neuron1 = np.maximum(0, adjusted_input_neuron1)\n",
    "\n",
    "# Step 4: Second neuron computation (taking output of Neuron 1 as input)\n",
    "adjusted_input_neuron2 = weight_neuron2 * relu_output_neuron1 + bias_neuron2\n",
    "relu_output_neuron2 = np.maximum(0, adjusted_input_neuron2)\n",
    "\n",
    "# Plot the outputs of both neurons\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(input_values, relu_output_neuron1, label='Output of Neuron 1', color='blue')# Neuron 1 output\n",
    "\n",
    "plt.plot(input_values, relu_output_neuron2, label='Output of Neuron 2', color='green') # Neuron 2 output\n",
    "\n",
    "# Axes and grid\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8) \n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)  \n",
    "\n",
    "# Labels and legend\n",
    "plt.title(\"Two Connected Neurons with ReLU Activation\", fontsize=14)\n",
    "plt.xlabel(\"Initial Input\", fontsize=12)\n",
    "plt.ylabel(\"Output\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting second neuron bias to 1 : \n",
    "\n",
    "# Step 1: Define the input range\n",
    "x_values = np.linspace(-5, 5, 100)  # Input from -5 to 5\n",
    "\n",
    "# Step 2: Compute Neuron 1 output\n",
    "w1 = -1  # weight of Neuron 1\n",
    "b1 = 0.5  # bias of Neuron 1\n",
    "z1 = w1 * x_values + b1  # Linear combination\n",
    "a1 = np.maximum(0, z1)  # Apply ReLU activation\n",
    "\n",
    "# Step 3: Compute Neuron 2 output\n",
    "w2 = 1  # weight of Neuron 2\n",
    "b2 = 1  # updated bias of Neuron 2\n",
    "z2 = w2 * a1 + b2  # Linear combination\n",
    "a2 = np.maximum(0, z2)  # Apply ReLU activation\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Neuron 1's output\n",
    "plt.plot(x_values, a1, label=\"Neuron 1 Output (a1)\", color=\"blue\", linestyle=\"--\")\n",
    "\n",
    "# Neuron 2's output\n",
    "plt.plot(x_values, a2, label=\"Neuron 2 Output (a2)\", color=\"red\")\n",
    "\n",
    "# Formatting the plot\n",
    "plt.axhline(0, color=\"black\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.axvline(0, color=\"black\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.title(\"Output of Two Neurons in Hidden Layers\")\n",
    "plt.xlabel(\"Input (x)\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdced7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the function\n",
    "\n",
    "#initialize the weights and bias\n",
    "\n",
    "w1 = -1.00\n",
    "b1 = 0.50\n",
    "w2 = 1.00\n",
    "b2 = 1.00\n",
    "\n",
    "#initializing the inputs\n",
    "inputs =np.linspace(-2, 2, 100)\n",
    "\n",
    "#defining the relu function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "#computing neuron1 output\n",
    "z1 = w1 * inputs + b1\n",
    "a1 = relu(z1) #apply relu activation\n",
    "\n",
    "#computing neuron 2 output\n",
    "z2 = w2 * a1 + b2\n",
    "a2 = relu(z2) #apply relu activation\n",
    "\n",
    "#plot the graph\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "#plot neuron1 output\n",
    "plt.plot(x_values, a1, label = \"neuron 1 output\", color = 'blue', linestyle = '--')\n",
    "\n",
    "#plot the neuron2 output\n",
    "plt.plot(x_values, a2, label = \"neuron 2 output\", color = \"red\" )\n",
    "\n",
    "#legends and plot\n",
    "plt.axhline(0, color = 'black', linewidth = 0.5, linestyle = \"--\")\n",
    "plt.axvline(0, color = \"black\", linewidth = 0.5, linestyle = \"--\")\n",
    "plt.title(\"Output of two neuron in hidden layer\")\n",
    "plt.xlabel('inputs(x)')\n",
    "plt.ylabel('outputs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73283ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: Initializing the inputs:\n",
    "w1 = -1.00\n",
    "b1 = 0.5\n",
    "w2 = -2.00\n",
    "b2 = 1.00\n",
    "\n",
    "#step 2: initializing the inputs:\n",
    "x_values = np.linspace(-2, 2, 100)\n",
    "\n",
    "#step3: compute neuron1 output\n",
    "z1 = w1 * x_values + b1\n",
    "a1 = np.maximum(0, z1) #applying the relu activation function\n",
    "\n",
    "#step4: compute the neuron2 output\n",
    "z2 = w2 * a1 + b2 # the output of first neuron is taken as input to the second one\n",
    "a2 = np.maximum(0, z2) #applying the relu activation function\n",
    "\n",
    "#plotting the graph\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "#first neuron output\n",
    "plt.plot(x_values , a1, label = \"first neuron output a1\", color = \"blue\", linestyle = \"--\")\n",
    "\n",
    "#second neuron output\n",
    "plt.plot(x_values, a2, label = \"second neuron output a2\", color = \"red\")\n",
    "\n",
    "plt.axhline(0, color = \"black\", linewidth = 0.5, linestyle = \"--\")\n",
    "plt.axvline(0, color = \"black\", linewidth = 0.5, linestyle = \"--\")\n",
    "\n",
    "plt.title(\"output after adjusting the weight of second neuron to -2\")\n",
    "plt.xlabel(\"Input(x)\")\n",
    "plt.ylabel(\"output\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeddad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU activation code:\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "\n",
    "\"\"\"\n",
    "i. the Relu in this code is below loop where we are checking if the current value > 0 ?\n",
    "ii. If it is, then we are appending to output variable\n",
    "iii. If not we are appending 0.\n",
    "\"\"\"\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else: \n",
    "        output.append(0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5411fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#another way:\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    " \n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(max(0, i))\n",
    "    else:\n",
    "        output.append(0)\n",
    "        \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy contains np.maximum(): compares each element of the input list (or an array) and returns an object of the same shape filled with new values.\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = np.maximum(0, inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05571cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rectified Linear activation used in hidden layer from scratch:\n",
    "\n",
    "#import the library\n",
    "import numpy as np\n",
    "\n",
    "# ReLU activation class\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)  # Corrected typo\n",
    "\n",
    "# Define the spiral data generation function\n",
    "def spiral_data(samples=100, classes=3):\n",
    "    \"\"\"\n",
    "    \n",
    "     1. spiral_data : This function generates a dataset of 2D points arranged in a spiral pattern, with labels for each class.\n",
    "     2. X - A 2D matrix of zeros with shape (sample * classes), which stores 2D coordinates of the points.\n",
    "     3. y - An array of zeros with shape(samples * classes), which will store class label 0, 1, 2 etc.\n",
    "    \"\"\"\n",
    "    X = np.zeros((samples * classes, 2))  # Features matrix (samples, 2)\n",
    "    y = np.zeros(samples * classes, dtype='uint8')  # Labels array\n",
    "\n",
    "    for class_number in range(classes):\n",
    "        \"\"\"\n",
    "    \n",
    "        For each class, compute:\n",
    "        Radius r:\n",
    "        Evenly spaced values between 0.0 and 1.0. Points start near the origin and move outward.\n",
    "\n",
    "        Theta t:\n",
    "        Angular values unique to the class, creating a spiral pattern.\n",
    "        Random noise (np.random.randn(samples) * 0.2) is added to make the pattern less perfect.\n",
    "\n",
    "        Cartesian Conversion:\n",
    "        Polar coordinates (r, t) are converted to Cartesian coordinates (x, y):\n",
    "        𝑥=𝑟⋅sin(𝑡),𝑦=𝑟⋅cos(𝑡)\n",
    "        x=r⋅sin(t),y=r⋅cos(t)\n",
    "        Store the computed points in X and assign class labels in y.\n",
    "        \"\"\"\n",
    "        ix = range(samples * class_number, samples * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, samples)  # radius\n",
    "        t = np.linspace(class_number * 4, (class_number + 1) * 4, samples) + np.random.randn(samples) * 0.2  # theta\n",
    "\n",
    "        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]  # Convert polar to cartesian(X = r * sin(t), Y = r * cos(t))\n",
    "        y[ix] = class_number  # Assign the class number (0, 1, or 2)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Dense Layer class\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        \n",
    "        __init__: Initializes the weights (random small values) and biases (zeros).\n",
    "        forward: Computes the output using the formula above, where inputs is the input data.\n",
    "        \"\"\"\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Create the dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create the dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Perform forward pass on the data through the dense layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Create ReLU activation to be used with dense layer\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make the forward pass of the dense layer's output through ReLU activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Print the first 5 outputs\n",
    "print(activation1.output[:5]) #the negative value have been clipped(modified to be zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c322175",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Plot the points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg', s=5)\n",
    "plt.title(\"Spiral Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using ReLU activation function to fit the sin function and visualizing\n",
    "\n",
    "#importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor # the MlPRegressor from sklearn is used to define the model with two hidden layer of 64 neuron each.\n",
    "\n",
    "# generating data \n",
    "X = np.linspace(0, 2 * np.pi, 500).reshape(-1, 1)  # Inputs\n",
    "y = np.sin(X).ravel()  # Sine function without noise\n",
    "\n",
    "# Visualizing data before fitting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, y, color=\"red\", label=\"sine function\")\n",
    "plt.title(\"Data before fitting\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Defining and training the model\n",
    "model = MLPRegressor(hidden_layer_sizes=(64, 64), activation='relu',\n",
    "                     max_iter=5000, solver='adam', random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "#making  Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Visualizing  data after fitting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, y, color=\"red\", label=\"sine function\")\n",
    "plt.plot(X, y_pred, color=\"blue\", label=\"Predicted function using ReLU\")\n",
    "plt.title(\"Data after fitting with ReLU-based Neural Network\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62b9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
