{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabacd26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_output = [4.8, 1.21, 2.385]\n",
    "\n",
    "#e = euler constant value 2.7182812846\n",
    "\n",
    "E = 2.7182812846\n",
    "\n",
    "#calculating the exponentian values for each vector\n",
    "exp_values = []\n",
    "for output in layer_output:\n",
    "    exp_values.append(E ** output)\n",
    "    \n",
    "print('exponential values: ', exp_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f61be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#adding the sum and normalization to the code:\n",
    "\n",
    "#normalizing the values\n",
    "\n",
    "norm_base = sum(exp_values) #sum all above exp_values- 121.51... + 3.35... + 10.85...\n",
    "#print(norm_base)\n",
    "\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "    \n",
    "print('Normalized expontial values: ', norm_values)\n",
    "print('Sum of normalized values: ', sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de140d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#using numpy:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('The exponential values is: ', exp_values)\n",
    "\n",
    "#normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(norm_values)\n",
    "\n",
    "print('Sum of normalized values: ', np.sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7e92d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                         [8.9, -1.81, 0.2],\n",
    "                         [1.41, 1.051, 0.026]])\n",
    "\n",
    "#axis1 - refers column and axis0 - refers rows\n",
    "print('The sum of axis 1 is : ')\n",
    "print(np.sum(layer_outputs, axis = 1, keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db752ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. With this we keep the same dimension as the input.\n",
    "2. now if we divide the array containing a batch of the outputs with this array, Numpy will perform this sample-wise.\n",
    "3. This means, divide all values from each output row by the corresponding row from the sum array.\n",
    "4. Since this sum in each row is a single value, it will be used for the division with every value from the corresponding output row.\n",
    "\"\"\"\n",
    "\n",
    "#combining this with softmax class like:\n",
    "\n",
    "#softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, layer_outputs):\n",
    "        # Stabilize computation by subtracting the maximum value in each row from the output\n",
    "        exp_values = np.exp(layer_outputs - np.max(layer_outputs, axis = 1))\n",
    "        \n",
    "        #normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "        \n",
    "#test the activation_softmax class\n",
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                          [8.9, -1.81, 0.2],\n",
    "                          [1.41, 1.051, 0.026]])\n",
    "\n",
    "#initialize the softmax activation\n",
    "activation = Activation_Softmax()\n",
    "\n",
    "#apply the forward pass\n",
    "activation.forward(layer_outputs)\n",
    "\n",
    "#print the computed softmax probabilities\n",
    "print(\"softmax probabilities:\\n \",activation.output)\n",
    "    \n",
    "#verifying the results\n",
    "print('If all 1, its verified:\\n', np.sum(activation.output, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb8831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two main pervasive challenges with neural network: dead neurons and exploding values\n",
    "They can wreak havoc down the line and render a network useless over time.\n",
    "\n",
    "\"\"\"\n",
    "print(np.exp(1))\n",
    "\n",
    "print(np.exp(10))\n",
    "\n",
    "print(np.exp(100))\n",
    "\n",
    "print(np.exp(1000)) # RuntimeWarning: overflow encountered in exp cause it exceed the range that can be represented by NumPy's floating-point numbers (usually float64).\n",
    "\n",
    "#this can be solved by following property:\n",
    "\"\"\"\n",
    "np.exp(-np.inf) mathematically evaluates to 0.\n",
    "\n",
    "np.exp(0) mathematically evaluates to 1.)\n",
    "\n",
    "\"\"\"\n",
    "print(np.exp(-np.inf), np.exp(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        #calculating the outputs from the inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "class Activation_Softmax:\n",
    "    #def forward pass\n",
    "    def forward(self, inputs):\n",
    "        #get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        \n",
    "        #normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "    \n",
    "#dense layer class\n",
    "class Dense_Layer:\n",
    "    #initializing the layer\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    #forward pass    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "#create the dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "#create the dense layer with 2 inputs and 3 output values\n",
    "dense1 = Dense_Layer(2, 3)\n",
    "\n",
    "#create the relu activation function to use with dense layer\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#create the second dense layer with 3 input features(here we take the output of previous layer) and 3 output values\n",
    "dense2 = Dense_Layer(3, 3)\n",
    "\n",
    "#creating the softmax activation to use with dense layer\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "#forward passing of our training through layer 1\n",
    "dense1.forward(X)\n",
    "\n",
    "#making the forward pass through ReLU activation function, here  it takes output of first dense layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "#making the forward pass through second layer- here it take output of ReLU activation as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "#making the forward pass through softmax activation which takes the output of second dense layer as input\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the spiral_data:\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg', s=5)\n",
    "plt.title(\"Spiral Data\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d813a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [4.8, 1.21, 2.385]\n",
    "exp_values = np.exp(values)\n",
    "print(exp_values)\n",
    "normalize = exp_values/ np.sum(exp_values)\n",
    "print(normalize)\n",
    "print(sum(normalize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
